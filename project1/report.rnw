\documentclass{article}
\usepackage{float}


\title{CSCI447: Analysis of Traditional Machine-Learning Algorithms on Real-World Data.}
\author{Christopher R. Barbour, Brandon Fenton, John Sherrill}

\date{\today}

 %% LaTeX margin settings:
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{10.05in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{0cm}
\setlength{\hoffset}{0cm}
\setlength{\voffset}{-1in}

 %% tell knitr to use smaller font for code chunks
\ifdefined\knitrout
\renewenvironment{knitrout}{\begin{footnotesize}}{\end{footnotesize}}
 \else
\fi
\newcommand{\R}{{\sf R}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bfD}{\mbox{\boldmath $D$}}
\newcommand{\bfR}{\mbox{\boldmath $R$}}
\newcommand{\bfmu}{\mbox{\boldmath $\mu$}}
\newcommand{\bfI}{\mbox{\boldmath $I$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}

\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\Sigma$}}

\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

<<setup, include=FALSE, cache=FALSE>>=
# this is equivalent to \SweaveOpts{...}
opts_chunk$set(fig.width=6.5, fig.height=5, out.width='\\linewidth', dev='pdf', concordance=TRUE)
options(replace.assign=TRUE,width=112, digits = 3, max.print="140",
        show.signif.stars = FALSE)
@

\begin{document}

\maketitle

\begin{center}
\abstract{Hello, this is our abstractjfslfksjlfksdjflksdj lskjdfsljf;asdkfja;lkdjfl slkdfj;aksdjflksdjf a;sldkjfalskdfja;kljdf a;lksdjf;aklsjdfl ksjd.}
\end{center}

\tableofcontents

\section{Introduction}

The purpose of this study is to assess the classification performance of 10 machine-learning algorithms on 10 datasets from the UCI repository of machine-learning datasets. The algorithms, listed in section 2.1, cover a broad spectrum of classification techniques with varying degrees of assumptions, computational intensity, and inductive biases. The latter of these will be the focus on our performance hypothesis of each dataset and interpretation of results.

\section{Experimental Design}

For each of the selected datasets and algorithms, we will randomly partition the examples into a training dataset which will be used to construct the classifier and a test dataset which will be used to test the classifiers ability to generalize, or correctly classify similar examples from the same population of interest. Each algorithm will be heuristically tuned to each dataset prior to the experiment using a single run 10-fold Cross-validation. The measures used to quantify this classification ability will be one minus the misclassification rate, the weighted average F-measure, and the weighted average Area under the ROC curve. These measures were chosen to assess different capabilities of each algorithm. Misclassification rate measures.... . Weighted Average F-measure related to .... . Weighted Average Area under the ROC assess.... . 

\subsection{Datasets}

The datasets selected were chosen to contain a classification problem, be free of missing attributes or classes, and be of somewhat interest to the authors. Prior to the experiment, variables with each example having a unique value (i.e. a sample identification, or name of an animal) was removed. Table 1 displays the name of our datasets with brief descriptions, the types of attributes present, the number of classes for prediction, and the number of total examples. 

\subsection{Algorithm Descriptions}

The algorithms implemented in the experiment are displayed below in Table 2. As stated earlier, individual tuning to each dataset was done prior to the experiment, and these results are displayed in the supplementary information. Some options remained fixed during all of the experiment, and these are discussed below along with certain examples of how tuning was performed.

\subsubsection{Simple nearest neighbor (IB1)}

These algorithms classify instances by way of referencing a set $X$ of classified instances. For a given instance $x$ of unknown class, the algorithm searches $X$ for an element $y$ that most closely resembles $x$ in attribute. Various metrics may be used for defining ``resemblance''. The class of $x$ is then deteremined to be the class of $y$.

Inductive biases: assuming that points close together are alike.

Reference.

\subsubsection{$K$-nearest neighbor (IBk)}

This is a generalization of the simple nearest neighbor algorithms. For instance $x$ to be classified, the $K$-nearest neighbor algorithm searches a reference set $X$ for the $K$ elements most closely resembling $x$ (again, this is dependent upon the metric choice). The class of $x$ is given by the most frequent class of the $K$ elements found.

Inductive biases: assuming that points close together are alike.

Reference.

\subsubsection{Naive Bayes}
\subsubsection{Logistic regression}
\subsubsection{Decision tree (J48)}

These algorithms classify instances by forming a cascading decision making mechanism in the form of a ``descision tree''. The tree is constructed such that attributes found to carry more information about the class of the instances are used earlier in the mechanism than those attributes found to carry less information. 

J48 is an open source Java implementation of Quinlan's C4.5 algorithm. The reference for that particular flavor of Decision tree algorithms is Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993.

Inductive biases may include: a preference for shorter trees, trees that place high information gain attributes close to the root are preferred over those that do not, selection of the first functioning tree (the learning algorithm is a greedy algorithm). Linear separability of domain (reference for this statement?)

\subsubsection{Ripper (JRip)}
\subsubsection{Support vector machine (LibSVM or SMO)}
\subsubsection{Feedforward neural networks (Multilayer Perceptron)}

These algorithms classify instances by forming a directed graph that contains no cycles, that is, the information passes through the constructed network in a forward only direction. The graph consists of connected nodes (neurons) that either engage or don't based upon signals presented to them from other neurons upstream. The signals are added in a weighted manner and the neuron fires if a specified threshold is met.

Back-propagation is the most common learning method for multi-layer perceptrons. Neurons feed other neurons by manner of weights and ideal weights are found by utilizing gradient descent on a specified loss function where the loss function is minimized over the space of possible weights.

Inductive biases include: starting point for gradient descent optimization, require linear separability of domain.

Reference?

\subsubsection{Kernel neural network (RBFNetwork)}
\subsubsection{Ensemble (Adaboost)}

\subsection{Hypothesis}

\section{Results}

\section{Discussion}

\section{References}

\begin{center}
  {\large\bf Supplementary Information}
\end{center}

\begin{center}
  {\large\bf Tuning Parameter Information}
\end{center}

\begin{center}
  {\large\bf Supplemental Plots of Results}
\end{center}

<<ref.label='one',echo=TRUE,eval=FALSE,cache=TRUE>>=
@

\end{document}
